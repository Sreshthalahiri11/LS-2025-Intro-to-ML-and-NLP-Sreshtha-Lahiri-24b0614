# -*- coding: utf-8 -*-
"""sentiment_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dznF15RNJKL7BHjl5_Hg53iJxgEUzcuz
"""

# Uninstall and delete all traces - this is done because I faced tremendous issues loading the dataset in google colab  
#!pip uninstall -y transformers datasets huggingface_hub fsspec
#!rm -rf /usr/local/lib/python3.11/dist-packages/transformers*
#!rm -rf /usr/local/lib/python3.11/dist-packages/datasets*
##!rm -rf /usr/local/lib/python3.11/dist-packages/huggingface_hub*
#!rm -rf /usr/local/lib/python3.11/dist-packages/fsspec*

#!pip install -U transformers==4.41.2 datasets==2.19.1

from datasets import load_dataset
dataset = load_dataset("imdb")
#here I wanted to load a smaller subset of the dataset as the runtime was about 2 hours long for me
train_data = dataset["train"].shuffle(seed=42).select(range(1000))  
test_data = dataset["test"].shuffle(seed=42).select(range(200))

# Exporting the  data to be trained and tested  to pandas to completely avoid NumPy/Array issues i was facing

train_df = train_data.to_pandas()
test_df = test_data.to_pandas()

# Now we extract data as lists since the tolist showed error before
train_texts = train_df["text"].tolist()
test_texts = test_df["text"].tolist()
train_labels = train_df["label"].tolist()
test_labels = test_df["label"].tolist()

#tokenization is here 
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)

# here im creating the pytorch dataset 
import torch
from torch.utils.data import Dataset

class IMDbDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.encodings["input_ids"][idx]),
            "attention_mask": torch.tensor(self.encodings["attention_mask"][idx]),
            "labels": torch.tensor(self.labels[idx])
        }

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)

from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)
# here we intialize and then further train model.
from transformers import BertForSequenceClassification, AdamW

#tqdm is added so that I could see the progress while the model trained 
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

# Training (1 epoch of the dtataset  for demo)
model.train()
for batch in tqdm(train_loader, desc="Training"):
    optimizer.zero_grad()
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()

print("Training complete")
# now we check the accuracy of the model we trained 
from sklearn.metrics import accuracy_score, f1_score

model.eval()
preds, true = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=-1)

        preds.extend(predictions.cpu().numpy())
        true.extend(labels.cpu().numpy())

print("Accuracy:", accuracy_score(true, preds))
print("F1 Score:", f1_score(true, preds))